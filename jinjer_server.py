#!/usr/bin/env python3
"""
jinjeråŒæœŸã‚µãƒ¼ãƒãƒ¼ â€” PWAã‹ã‚‰ã®TailscaleçµŒç”±ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¿œç­”ã™ã‚‹è»½é‡HTTPã‚µãƒ¼ãƒãƒ¼ã€‚

ä½¿ã„æ–¹:
  python3 jinjer_server.py          # ãƒãƒ¼ãƒˆ 8899 ã§èµ·å‹•
  python3 jinjer_server.py 9000     # ãƒãƒ¼ãƒˆæŒ‡å®š

ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ:
  GET  /api/jinjer?months=2026-02        jinjer å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å–å¾—
  GET  /api/reports                      ä½œæ¥­å ±å‘Šæ›¸ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
  GET  /api/reports/read?year=2026&month=02  æŒ‡å®šæœˆ Excel â†’ JSON
  POST /api/reports/sync                 kintai ãƒ‡ãƒ¼ã‚¿ â†’ Excel æ›¸ãè¾¼ã¿
  POST /api/reports/generate             ç¿Œæœˆ Excel è‡ªå‹•ç”Ÿæˆ
  GET  /api/structure                    STRUCTURE.md ã®å†…å®¹
  GET  /api/jobs?categories=1,2,3&keywords=Python&platforms=crowdworks,lancers  æ¡ˆä»¶ä¸€è¦§

å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
  pip install playwright openpyxl
  playwright install chromium
"""
import asyncio
import errno as _errno
import json
import os
import re
import signal as _signal
import subprocess
import sys
import threading
import time
import urllib.request
import xml.etree.ElementTree as ET
import shutil
from datetime import datetime as _dt, timedelta as _td
from http.server import BaseHTTPRequestHandler, HTTPServer
from socketserver import ThreadingMixIn
from urllib.parse import urlparse, parse_qs
from pathlib import Path

_HERE = Path(__file__).parent
sys.path.insert(0, str(_HERE))

try:
    from sync_jinjer import scrape_months, convert_all, save_to_icloud_and_local
    _SCRAPER_OK = True
except ImportError as e:
    print(f'[ERROR] sync_jinjer.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}')
    _SCRAPER_OK = False

try:
    from report_sync import (
        list_reports, read_report,
        write_report_from_kintai, create_next_month_report,
    )
    _REPORT_OK = True
except (ImportError, SystemExit) as e:
    print(f'[WARN] report_sync.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•— (å ±å‘Šæ›¸æ©Ÿèƒ½ã¯ç„¡åŠ¹): {e}')
    _REPORT_OK = False

PORT = int(sys.argv[1]) if len(sys.argv) > 1 else 8899
_START_TIME = time.time()           # uptime è¨ˆç®—ç”¨
_cache = {}   # cache_key â†’ { ts, data }
CACHE_TTL = 300  # 5åˆ†

# Cloudflare Quick Tunnel â€” ç¾åœ¨ã®URL ã‚’ä¿æŒã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_CF_TUNNEL_URL: str = ''
_CF_TUNNEL_PROC = None
# ===== iCloud Drive ãƒ‘ã‚¹å®šç¾© =====
# `:root` ãƒ•ã‚©ãƒ«ãƒ€ = iCloud Drive ã®ãƒ«ãƒ¼ãƒˆï¼ˆMac/iPhone ä¸¡æ–¹ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰
_ICLOUD_ROOT = Path.home() / 'Library/Mobile Documents/com~apple~CloudDocs/:root'
ICLOUD_ATT_DIR    = _ICLOUD_ROOT / 'attendance'          # å‹¤æ€ ã‚¢ãƒ—ãƒªçµ±åˆãƒ•ã‚©ãƒ«ãƒ€
ICLOUD_JINJER_DIR = ICLOUD_ATT_DIR / 'jinjer'            # jinjeråŒæœŸãƒ•ã‚¡ã‚¤ãƒ«ç½®ãå ´
ICLOUD_BACKUP_DIR = ICLOUD_ATT_DIR / 'Backup'            # ä¸–ä»£ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç½®ãå ´
# æ—§ãƒ‘ã‚¹ (äº’æ›æ€§ã®ãŸã‚ä¿æŒ)
ICLOUD_DIR = Path.home() / 'Library/Mobile Documents/com~apple~CloudDocs/kintai'
STRUCTURE_MD = _HERE / 'STRUCTURE.md'

# ===== ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒªãƒƒã‚¸ =====
# GitHub Pages / localhost / iPhone ãªã©ç•°ãªã‚‹ã‚ªãƒªã‚¸ãƒ³é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰ã™ã‚‹ãŸã‚ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆã‚¢ã€‚
# localStorage ã¯ã‚ªãƒªã‚¸ãƒ³ã”ã¨ã«åˆ†é›¢ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚µãƒ¼ãƒãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ©‹æ¸¡ã—ã«ãªã‚‹ã€‚
DATA_DIR = _HERE / 'data'
DATA_DIR.mkdir(exist_ok=True)
KINTAI_DATA_FILE = DATA_DIR / 'kintai_store.json'   # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿
TASKS_DATA_FILE  = DATA_DIR / 'kintai_tasks.json'   # ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
# â€» data/ ã¯ .gitignore ã§é™¤å¤–ã™ã‚‹ã“ã¨ï¼ˆå€‹äººãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ï¼‰

# ===== ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹æ¡ˆä»¶ãƒ•ã‚£ãƒ¼ãƒ‰ =====
JOBS_CACHE_TTL = 3600  # 1æ™‚é–“
_jobs_cache: dict = {}

# Crowdworks ã‚«ãƒ†ã‚´ãƒª ID â†’ è¡¨ç¤ºå
CW_CATEGORIES = {
    '1':  'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º',
    '2':  'Webåˆ¶ä½œãƒ»Webãƒ‡ã‚¶ã‚¤ãƒ³',
    '3':  'ã‚¹ãƒãƒ›ã‚¢ãƒ—ãƒªé–‹ç™º',
    '7':  'ECã‚µã‚¤ãƒˆæ§‹ç¯‰',
    '9':  'ãƒ‡ãƒ¼ã‚¿å…¥åŠ›',
}
# Lancers work_type â†’ è¡¨ç¤ºå
LANCERS_TYPES = {
    'system': 'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º',
    'web':    'Webåˆ¶ä½œ',
    'app':    'ã‚¢ãƒ—ãƒªé–‹ç™º',
}
_JOBS_UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (kintai-pwa/1.0)'
_ATOM_NS = 'http://www.w3.org/2005/Atom'


def _atom_text(entry, tag: str) -> str:
    return entry.findtext(f'{{{_ATOM_NS}}}{tag}', '') or ''


def _parse_atom_feed(data: bytes, platform: str, category_label: str) -> list:
    """Atom XML ãƒã‚¤ãƒˆåˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ¡ˆä»¶ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    root = ET.fromstring(data)
    jobs = []
    for entry in root.findall(f'{{{_ATOM_NS}}}entry'):
        title   = _atom_text(entry, 'title').strip()
        link_el = entry.find(f'{{{_ATOM_NS}}}link')
        url     = (link_el.get('href', '') if link_el is not None else '')
        summary = (_atom_text(entry, 'summary') or _atom_text(entry, 'content'))
        updated = _atom_text(entry, 'updated')
        uid     = _atom_text(entry, 'id') or url
        # äºˆç®—æŠ½å‡ºï¼ˆæ•°å­—+å††ï¼‰
        budget = ''
        m = re.search(r'([\d,]+)\s*å††', summary)
        if m:
            budget = f"Â¥{m.group(1)}"
        # HTMLã‚¿ã‚°é™¤å»ã—ã¦çŸ­ã„è¦ç´„ã‚’ä½œæˆ
        clean = re.sub(r'<[^>]+>', '', summary)[:160].strip()
        jobs.append({
            'platform':    platform,
            'category':    category_label,
            'title':       title,
            'url':         url,
            'summary':     clean,
            'budget':      budget,
            'updated':     updated,
            'id':          uid,
            'match_score': 0,
        })
    return jobs


def _fetch_cw_feed(cat_id: str) -> list:
    """Crowdworks ã‚«ãƒ†ã‚´ãƒª Atom ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹"""
    url = f'https://crowdworks.jp/public/jobs/category/{cat_id}.atom'
    try:
        req = urllib.request.Request(url, headers={'User-Agent': _JOBS_UA})
        with urllib.request.urlopen(req, timeout=12) as res:
            data = res.read()
        return _parse_atom_feed(data, 'crowdworks', CW_CATEGORIES.get(cat_id, cat_id))
    except Exception as e:
        print(f'[jobs] CW cat={cat_id}: {e}')
        return []


def _fetch_lancers_feed(work_type: str) -> list:
    """Lancers æ¤œç´¢ Atom ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹"""
    url = f'https://www.lancers.jp/work/search.atom?work_type[]={work_type}&order=new'
    try:
        req = urllib.request.Request(url, headers={'User-Agent': _JOBS_UA})
        with urllib.request.urlopen(req, timeout=12) as res:
            data = res.read()
        return _parse_atom_feed(data, 'lancers', LANCERS_TYPES.get(work_type, work_type))
    except Exception as e:
        print(f'[jobs] Lancers type={work_type}: {e}')
        return []


def _save_to_icloud(target_months: list, pwa_data: dict):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—çµæœã‚’ iCloud Drive ã® kintai ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã™ã‚‹ (æ—§äº’æ›)"""
    try:
        ICLOUD_DIR.mkdir(parents=True, exist_ok=True)
        if len(target_months) == 1:
            filename = f'jinjer_sync_{target_months[0]}.json'
        else:
            filename = f'jinjer_sync_{target_months[0]}_to_{target_months[-1]}.json'
        content = json.dumps(pwa_data, ensure_ascii=False, indent=2)
        icloud_path = ICLOUD_DIR / filename
        icloud_path.write_text(content, encoding='utf-8')
        print(f'â˜ï¸  iCloud Drive (æ—§) â†’ {icloud_path}')
    except Exception as e:
        print(f'âš ï¸  iCloud Driveã¸ã®ä¿å­˜å¤±æ•—: {e}')


def _icloud_backup(kintai_data: dict, label: str = '') -> dict:
    """kintai å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’ iCloud Drive ã® :root/attendance/ ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã™ã‚‹ã€‚

    ä¿å­˜å…ˆ:
      attendance/attendance_backup.json          â† å¸¸ã«æœ€æ–°ç‰ˆ (æ—¢å­˜ app.py ã¨åŒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
      attendance/Backup/attendance_backup_YYYYMMDD_HHMMSS.json â† ä¸–ä»£ç®¡ç† (æœ€å¤§30ä»¶)

    Returns:
      {'ok': bool, 'path': str, 'backup_path': str, 'error': str}
    """
    ts = _dt.now().strftime('%Y%m%d_%H%M%S')
    result = {'ok': False, 'ts': ts}
    try:
        ICLOUD_ATT_DIR.mkdir(parents=True, exist_ok=True)
        ICLOUD_BACKUP_DIR.mkdir(parents=True, exist_ok=True)

        # ãƒ¡ã‚¿æƒ…å ±ã‚’é™¤ã„ãŸã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ (attendance_backup.json äº’æ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
        clean = {k: v for k, v in kintai_data.items()
                 if k not in ('_server_saved_at', '_server_updated_at')}

        content = json.dumps(clean, ensure_ascii=False, indent=2)

        # â”€â”€ ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— (attendance_backup.json) ã‚’ä¸Šæ›¸ã â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        main_path = ICLOUD_ATT_DIR / 'attendance_backup.json'
        main_path.write_text(content, encoding='utf-8')
        print(f'â˜ï¸  iCloud backup â†’ {main_path}')

        # â”€â”€ ä¸–ä»£ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— (Backup/YYYYMMDD_HHMMSS.json) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        bak_name  = f'attendance_backup_{ts}.json'
        bak_path  = ICLOUD_BACKUP_DIR / bak_name
        bak_path.write_text(content, encoding='utf-8')
        print(f'â˜ï¸  iCloud Backup  â†’ {bak_path}')

        # å¤ã„ä¸–ä»£ã‚’ 30 ä»¶ã«åˆ¶é™
        baks = sorted(ICLOUD_BACKUP_DIR.glob('attendance_backup_*.json'),
                      key=lambda p: p.stat().st_mtime)
        for old in baks[:-30]:
            old.unlink(missing_ok=True)

        result.update({'ok': True, 'main': str(main_path), 'backup': str(bak_path)})
    except Exception as e:
        print(f'âš ï¸  iCloudãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}')
        result['error'] = str(e)
    return result


class ReuseHTTPServer(ThreadingMixIn, HTTPServer):
    """ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ + SO_REUSEADDR HTTPServer
    ThreadingMixIn: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç† â†’ jinjeråŒæœŸä¸­ã‚‚ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãŒå¿œç­”ã§ãã‚‹
    """
    allow_reuse_address = True
    daemon_threads = True  # ã‚µãƒ¼ãƒãƒ¼åœæ­¢æ™‚ã«ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å¼·åˆ¶çµ‚äº†


class JinjerHandler(BaseHTTPRequestHandler):
    def log_message(self, fmt, *args):
        print(f'[jinjer_server] {fmt % args}')

    def _send_json(self, data, status=200):
        body = json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8')
        self.send_response(status)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.send_header('Content-Length', str(len(body)))
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
        self.wfile.write(body)

    def _send_text(self, text: str, status=200):
        body = text.encode('utf-8')
        self.send_response(status)
        self.send_header('Content-Type', 'text/plain; charset=utf-8')
        self.send_header('Content-Length', str(len(body)))
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(body)

    def _read_body(self) -> dict:
        """POST ãƒœãƒ‡ã‚£ã‚’ JSON ã¨ã—ã¦èª­ã¿è¾¼ã‚€"""
        length = int(self.headers.get('Content-Length', 0))
        if length == 0:
            return {}
        raw = self.rfile.read(length)
        try:
            return json.loads(raw.decode('utf-8'))
        except Exception:
            return {}

    def do_OPTIONS(self):
        self.send_response(204)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()

    def do_GET(self):
        parsed = urlparse(self.path)
        path   = parsed.path
        params = parse_qs(parsed.query)

        # ===== /api/health =====
        if path == '/api/health':
            self._send_json({
                'status':          'ok',
                'uptime_seconds':  int(time.time() - _START_TIME),
                'report':          _REPORT_OK,
                'scraper':         _SCRAPER_OK,
            })

        # ===== /api/jinjer =====
        elif path == '/api/jinjer':
            self._handle_jinjer(params)

        # ===== /api/reports =====
        elif path == '/api/reports':
            if not _REPORT_OK:
                self._send_json({'error': 'report_sync.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}, 500)
                return
            self._send_json(list_reports())

        # ===== /api/reports/read =====
        elif path == '/api/reports/read':
            if not _REPORT_OK:
                self._send_json({'error': 'report_sync.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}, 500)
                return
            year  = params.get('year',  [''])[0]
            month = params.get('month', [''])[0]
            if not year or not month:
                self._send_json({'error': 'year ã¨ month ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™'}, 400)
                return
            data = read_report(year, month)
            if data is None:
                self._send_json({'error': f'{year}å¹´{month}æœˆã® Excel ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}, 404)
                return
            self._send_json(data)

        # ===== /api/structure =====
        elif path == '/api/structure':
            if STRUCTURE_MD.exists():
                self._send_text(STRUCTURE_MD.read_text(encoding='utf-8'))
            else:
                self._send_text('STRUCTURE.md ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚generate_structure.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚', 404)

        # ===== /api/jobs =====
        elif path == '/api/jobs':
            self._handle_jobs(params)

        # ===== /api/files =====
        elif path == '/api/files':
            self._handle_files_list()

        # ===== /api/files/read =====
        elif path == '/api/files/read':
            self._handle_files_read(params)

        # ===== /api/system/logs =====
        elif path == '/api/system/logs':
            self._handle_system_logs(params)

        # ===== /api/system/logs/dates â€” åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ =====
        elif path == '/api/system/logs/dates':
            self._handle_log_dates()

        # ===== /api/kintai-data â€” ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾— =====
        elif path == '/api/kintai-data':
            self._handle_data_get(KINTAI_DATA_FILE)

        # ===== /api/tasks-data â€” ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾— =====
        elif path == '/api/tasks-data':
            self._handle_data_get(TASKS_DATA_FILE)

        # ===== /api/tailscale-url â€” Tailscale Serve ã® HTTPS URL ã‚’è¿”ã™ =====
        elif path == '/api/tailscale-url':
            self._handle_tailscale_url()

        # ===== /api/tunnel-url â€” Cloudflare Quick Tunnel ã®ç¾åœ¨ URL ã‚’è¿”ã™ =====
        elif path == '/api/tunnel-url':
            self._handle_tunnel_url()

        # ===== /api/docker/containers â€” ã‚³ãƒ³ãƒ†ãƒŠä¸€è¦§ =====
        elif path == '/api/docker/containers':
            self._handle_docker_containers()

        # ===== /api/docker/images â€” ã‚¤ãƒ¡ãƒ¼ã‚¸ä¸€è¦§ =====
        elif path == '/api/docker/images':
            self._handle_docker_images()

        # ===== /api/docker/logs â€” ã‚³ãƒ³ãƒ†ãƒŠãƒ­ã‚° =====
        elif path == '/api/docker/logs':
            self._handle_docker_logs(params)

        # ===== /api/backup/list â€” iCloud ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§ =====
        elif path == '/api/backup/list':
            self._handle_backup_list()

        # ===== /api/backup/read â€” ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å†…å®¹å–å¾— =====
        elif path == '/api/backup/read':
            self._handle_backup_read(params)

        # ===== é™çš„ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡ â€” http://localhost:8899/ ã§ PWA ã‚’ç›´æ¥è¡¨ç¤º =====
        # Safari ã¯ HTTPS(GitHub Pages) â†’ HTTP(localhost) ã®æ··åœ¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹ãŸã‚ã€
        # Mac ã§ã¯ http://localhost:8899/ ã‚’ç›´æ¥é–‹ãã“ã¨ã§åŒä¸€ã‚ªãƒªã‚¸ãƒ³ã«ãªã‚Šãƒ–ãƒ­ãƒƒã‚¯ã‚’å›é¿ã§ãã‚‹ã€‚
        elif path in ('/', '/index.html'):
            self._send_static('index.html')
        elif path in ('/sw.js', '/manifest.json', '/recover.html',
                      '/icon-apple.png', '/icon-192.png', '/icon-512.png'):
            self._send_static(path)

        else:
            self._send_json({'error': 'Not found'}, 404)

    def do_POST(self):
        parsed = urlparse(self.path)
        path   = parsed.path

        # ===== /api/kintai-data â€” ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿ä¿å­˜ =====
        if path == '/api/kintai-data':
            self._handle_data_post(KINTAI_DATA_FILE)

        # ===== /api/tasks-data â€” ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ä¿å­˜ =====
        elif path == '/api/tasks-data':
            self._handle_data_post(TASKS_DATA_FILE)

        # ===== /api/reports/sync =====
        elif path == '/api/reports/sync':
            if not _REPORT_OK:
                self._send_json({'error': 'report_sync.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}, 500)
                return
            body  = self._read_body()
            year  = str(body.get('year', ''))
            month = str(body.get('month', ''))
            kdata = body.get('kintai_data', {})
            if not year or not month or not kdata:
                self._send_json({'error': 'year, month, kintai_data ãŒå¿…è¦ã§ã™'}, 400)
                return
            month = month.zfill(2)
            result = write_report_from_kintai(year, month, kdata)
            self._send_json(result, 200 if result['ok'] else 500)

        # ===== /api/reports/generate =====
        elif path == '/api/reports/generate':
            if not _REPORT_OK:
                self._send_json({'error': 'report_sync.py ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}, 500)
                return
            body  = self._read_body()
            year  = str(body.get('year', ''))
            month = str(body.get('month', ''))
            if not year or not month:
                self._send_json({'error': 'year ã¨ month ãŒå¿…è¦ã§ã™'}, 400)
                return
            month = month.zfill(2)
            result = create_next_month_report(year, month)
            self._send_json(result, 200 if result['ok'] else 400)

        # ===== /api/docker/action â€” ã‚³ãƒ³ãƒ†ãƒŠæ“ä½œ (start/stop/restart/rm) =====
        elif path == '/api/docker/action':
            self._handle_docker_action()

        # ===== /api/system/restart =====
        elif path == '/api/system/restart':
            self._handle_system_restart()

        # ===== /api/backup/now â€” æ‰‹å‹• iCloud ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ =====
        elif path == '/api/backup/now':
            self._handle_backup_now()

        # ===== /api/backup/restore â€” ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ kintai ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ =====
        elif path == '/api/backup/restore':
            self._handle_backup_restore()

        else:
            self._send_json({'error': 'Not found'}, 404)

    # ===== å†…éƒ¨: ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹æ¡ˆä»¶ãƒ•ã‚£ãƒ¼ãƒ‰ =====
    def _handle_jobs(self, params: dict):
        platforms_str  = params.get('platforms',  ['crowdworks,lancers'])[0]
        categories_str = params.get('categories', ['1,2,3'])[0]
        keywords_str   = params.get('keywords',   [''])[0]

        platforms  = [p.strip() for p in platforms_str.split(',')  if p.strip()]
        cat_ids    = [c.strip() for c in categories_str.split(',') if c.strip()]
        keywords   = [k.strip().lower() for k in keywords_str.split(',') if k.strip()]

        cache_key = f"{','.join(sorted(platforms))}|{','.join(sorted(cat_ids))}|{keywords_str}"
        if cache_key in _jobs_cache and time.time() - _jobs_cache[cache_key]['ts'] < JOBS_CACHE_TTL:
            print(f'[jobs cache hit] {cache_key}')
            self._send_json(_jobs_cache[cache_key]['data'])
            return

        all_jobs: list = []
        if 'crowdworks' in platforms:
            for cat in cat_ids:
                all_jobs.extend(_fetch_cw_feed(cat))
        if 'lancers' in platforms:
            for wtype in ['system', 'web', 'app']:
                all_jobs.extend(_fetch_lancers_feed(wtype))

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if keywords:
            for job in all_jobs:
                text = (job['title'] + ' ' + job['summary']).lower()
                job['match_score'] = sum(1 for kw in keywords if kw in text)
            all_jobs.sort(key=lambda j: (-j['match_score'], j.get('updated', '')))
        else:
            all_jobs.sort(key=lambda j: j.get('updated', ''), reverse=True)

        result = {
            'jobs':       all_jobs,
            'total':      len(all_jobs),
            'fetched_at': _dt.now().isoformat(),
        }
        _jobs_cache[cache_key] = {'ts': time.time(), 'data': result}
        self._send_json(result)

    # ===== å†…éƒ¨: jinjer åŒæœŸ =====
    def _handle_jinjer(self, params: dict):
        if not _SCRAPER_OK:
            self._send_json({'error': 'sync_jinjer.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¦ã„ã¾ã™'}, 500)
            return

        months_str = params.get('months', [''])[0]
        if not months_str:
            from datetime import date
            months_str = date.today().strftime('%Y-%m')

        target_months = [m.strip() for m in months_str.split(',') if m.strip()]
        cache_key = ','.join(sorted(target_months))

        if cache_key in _cache and time.time() - _cache[cache_key]['ts'] < CACHE_TTL:
            print(f'[cache hit] {cache_key}')
            self._send_json(_cache[cache_key]['data'])
            return

        print(f'[scrape] {target_months}')
        try:
            all_rows = asyncio.run(scrape_months(target_months))
            pwa_data = convert_all(all_rows)
            _cache[cache_key] = {'ts': time.time(), 'data': pwa_data}
            # iCloud + ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ï¼ˆæ”¹å–„ç‰ˆé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
            try:
                save_to_icloud_and_local(target_months, pwa_data)
            except Exception as save_e:
                print(f'[WARN] iCloudä¿å­˜å¤±æ•—: {save_e}')
            self._send_json(pwa_data)
        except Exception as e:
            print(f'[ERROR] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—å¤±æ•—: {e}')
            import traceback; traceback.print_exc()
            self._send_json({'error': str(e)}, 500)

    # ===== ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ =====
    def _handle_files_list(self):
        _EXCLUDE_DIRS  = {'.git', '__pycache__', 'node_modules', '.DS_Store'}
        _EXCLUDE_EXTS  = {'.pyc', '.pyo'}
        root = _HERE.resolve()
        files = []
        try:
            for item in sorted(root.rglob('*')):
                rel   = item.relative_to(root)
                parts = rel.parts
                # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã¯å…¨ã‚¹ã‚­ãƒƒãƒ—
                if any(p in _EXCLUDE_DIRS or p.startswith('.') and p != '.env' and p != '.gitignore'
                       for p in parts[:-1]):
                    continue
                if item.name in _EXCLUDE_DIRS:
                    continue
                if item.suffix in _EXCLUDE_EXTS:
                    continue
                try:
                    stat = item.stat()
                    files.append({
                        'path':     str(rel).replace('\\', '/'),
                        'is_dir':   item.is_dir(),
                        'size':     0 if item.is_dir() else stat.st_size,
                        'modified': _dt.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M'),
                    })
                except OSError:
                    pass
        except Exception as e:
            self._send_json({'error': str(e)}, 500)
            return
        self._send_json({'files': files})

    # ===== ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å–å¾— =====
    def _handle_files_read(self, params: dict):
        file_path = params.get('path', [''])[0]
        if not file_path:
            self._send_json({'error': 'path ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™'}, 400)
            return
        # ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«é˜²æ­¢
        try:
            target = (_HERE / file_path).resolve()
            target.relative_to(_HERE.resolve())
        except (ValueError, OSError):
            self._send_json({'error': 'ä¸æ­£ãªãƒ‘ã‚¹ã§ã™'}, 403)
            return
        if not target.exists() or target.is_dir():
            self._send_json({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}, 404)
            return
        # ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã¯è¿”ã•ãªã„
        _BINARY_EXTS = {'.png', '.jpg', '.jpeg', '.gif', '.ico', '.woff', '.woff2',
                        '.ttf', '.eot', '.pdf', '.xlsx', '.xls', '.zip', '.gz'}
        if target.suffix.lower() in _BINARY_EXTS:
            self._send_json({'error': 'ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“'}, 400)
            return
        _MAX = 200 * 1024  # 200KB
        try:
            raw = target.read_text(encoding='utf-8', errors='replace')
            truncated = len(raw) > _MAX
            self._send_json({
                'path':      file_path,
                'content':   raw[:_MAX],
                'size':      target.stat().st_size,
                'truncated': truncated,
            })
        except Exception as e:
            self._send_json({'error': str(e)}, 500)

    def _handle_system_logs(self, params: dict):
        """ç›´è¿‘ãƒ­ã‚°ã‚’è¿”ã™ã€‚type=server|watchdog, lines=N, date=YYYY-MM-DD"""
        log_type = params.get('type', ['server'])[0]
        lines_n  = min(int(params.get('lines', ['100'])[0]), 1000)
        date_str = (params.get('date', [''])[0]).strip()

        if date_str:
            # æ—¥ä»˜æŒ‡å®š: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰å–å¾—
            log_file_name = 'watchdog.log' if log_type == 'watchdog' else 'server.log'
            log_file = _LOG_ARCHIVE_DIR / date_str / log_file_name
        else:
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«
            log_file_name = 'watchdog.log' if log_type == 'watchdog' else 'server.log'
            log_file = _LOG_DIR / log_file_name

        if not log_file.exists():
            self._send_json({'error': 'ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“', 'type': log_type, 'date': date_str or 'latest'}, 404)
            return
        all_lines = log_file.read_text(encoding='utf-8', errors='replace').splitlines()
        recent = all_lines[-lines_n:] if len(all_lines) > lines_n else all_lines
        self._send_json({'type': log_type, 'date': date_str or 'latest', 'lines': recent, 'total': len(all_lines)})

    def _handle_log_dates(self):
        """åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ã‚’è¿”ã™"""
        dates = []
        if _LOG_ARCHIVE_DIR.exists():
            for d in sorted(_LOG_ARCHIVE_DIR.iterdir(), reverse=True):
                if d.is_dir() and re.match(r'^\d{4}-\d{2}-\d{2}$', d.name):
                    files = [f.name for f in d.iterdir() if f.is_file()]
                    dates.append({'date': d.name, 'files': files})
        self._send_json({'dates': dates, 'archive_dir': str(_LOG_ARCHIVE_DIR), 'count': len(dates)})

    # ===== é™çš„ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡ =====
    _MIME_MAP = {
        '.html': 'text/html; charset=utf-8',
        '.js':   'application/javascript; charset=utf-8',
        '.json': 'application/json; charset=utf-8',
        '.css':  'text/css; charset=utf-8',
        '.png':  'image/png',
        '.jpg':  'image/jpeg',
        '.jpeg': 'image/jpeg',
        '.svg':  'image/svg+xml',
        '.ico':  'image/x-icon',
        '.woff2': 'font/woff2',
        '.woff':  'font/woff',
        '.txt':   'text/plain; charset=utf-8',
        '.webmanifest': 'application/manifest+json',
    }

    def _send_static(self, rel_path: str):
        """é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ä¿¡ã™ã‚‹ (ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«å¯¾ç­–æ¸ˆã¿)"""
        try:
            target = (_HERE / rel_path.lstrip('/')).resolve()
            target.relative_to(_HERE.resolve())  # ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«é˜²æ­¢
        except (ValueError, OSError):
            self._send_json({'error': 'Forbidden'}, 403)
            return
        if not target.exists() or target.is_dir():
            self._send_json({'error': 'Not found'}, 404)
            return
        mime = self._MIME_MAP.get(target.suffix.lower(), 'application/octet-stream')
        try:
            body = target.read_bytes()
            self.send_response(200)
            self.send_header('Content-Type', mime)
            self.send_header('Content-Length', str(len(body)))
            # SW / ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ï¼ˆå¸¸ã«æœ€æ–°ã‚’ä½¿ç”¨ï¼‰
            if target.name in ('sw.js', 'manifest.json'):
                self.send_header('Cache-Control', 'no-cache, no-store, must-revalidate')
            else:
                self.send_header('Cache-Control', 'max-age=3600')
            self.end_headers()
            self.wfile.write(body)
        except Exception as e:
            self._send_json({'error': str(e)}, 500)

    # ===== ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒªãƒƒã‚¸: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿æ›¸ã =====
    def _handle_data_get(self, data_file: Path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ç©ºã‚’è¿”ã™ã€‚"""
        if data_file.exists():
            try:
                raw = data_file.read_text(encoding='utf-8')
                data = json.loads(raw)
                # æœ€çµ‚æ›´æ–°æ™‚åˆ»ã‚’ä»˜ä¸
                stat = data_file.stat()
                data['_server_updated_at'] = _dt.fromtimestamp(stat.st_mtime).isoformat()
                self._send_json(data)
                return
            except Exception as e:
                self._send_json({'error': str(e)}, 500)
                return
        # ãƒ•ã‚¡ã‚¤ãƒ«æœªå­˜åœ¨ â†’ ç©ºãƒ‡ãƒ¼ã‚¿è¿”å´
        self._send_json({'months': {}, '_server_updated_at': None})

    def _handle_data_post(self, data_file: Path):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜ã™ã‚‹ã€‚è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ãã€‚"""
        body = self._read_body()
        if not body:
            self._send_json({'error': 'ãƒœãƒ‡ã‚£ãŒç©ºã§ã™'}, 400)
            return
        try:
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— (æœ€å¤§3ä¸–ä»£)
            if data_file.exists():
                bak = data_file.with_suffix(f'.bak{int(time.time())}')
                data_file.rename(bak)
                # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’3ä¸–ä»£ã‚’è¶…ãˆãŸã‚‰å‰Šé™¤
                baks = sorted(data_file.parent.glob(data_file.stem + '.bak*'), key=lambda p: p.stat().st_mtime)
                for old in baks[:-3]:
                    old.unlink(missing_ok=True)
            body['_server_saved_at'] = _dt.now().isoformat()
            data_file.write_text(json.dumps(body, ensure_ascii=False, indent=2), encoding='utf-8')
            # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ iCloud ã«ã‚‚è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            icloud_result = {}
            if data_file == KINTAI_DATA_FILE and body.get('months'):
                icloud_result = _icloud_backup(body)
            self._send_json({'ok': True, 'saved_at': body['_server_saved_at'],
                             'icloud': icloud_result})
        except Exception as e:
            self._send_json({'error': str(e)}, 500)

    def _handle_tailscale_url(self):
        """Tailscale Serve ã® HTTPS URL ã‚’æ¤œå‡ºã—ã¦è¿”ã™ã€‚
        1. `tailscale serve status --json` ã§ serve è¨­å®šã‚’ç¢ºèª
        2. ãªã‘ã‚Œã° `tailscale status --json` ã§ãƒ›ã‚¹ãƒˆåã‚’å–å¾—ã—ã¦ URL ã‚’æ§‹ç¯‰
        """
        https_url = None
        method = 'none'

        # â”€â”€ æ–¹æ³•1: serve è¨­å®šã‹ã‚‰ç›´æ¥å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            r = subprocess.run(
                ['tailscale', 'serve', 'status', '--json'],
                capture_output=True, text=True, timeout=5
            )
            if r.returncode == 0 and r.stdout.strip():
                data = json.loads(r.stdout)
                # SelfDNS ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ HTTPS URL ã‚’æ§‹ç¯‰
                dns = (data.get('Self', {}).get('DNSName', '') or '').rstrip('.')
                if dns:
                    https_url = f'https://{dns}'
                    method = 'serve-status'
        except Exception:
            pass

        # â”€â”€ æ–¹æ³•2: tailscale status ã‹ã‚‰ãƒ›ã‚¹ãƒˆåã‚’å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not https_url:
            try:
                r2 = subprocess.run(
                    ['tailscale', 'status', '--json'],
                    capture_output=True, text=True, timeout=5
                )
                if r2.returncode == 0 and r2.stdout.strip():
                    data2 = json.loads(r2.stdout)
                    dns2 = (data2.get('Self', {}).get('DNSName', '') or '').rstrip('.')
                    if dns2:
                        https_url = f'https://{dns2}'
                        method = 'status'
            except Exception:
                pass

        if https_url:
            self._send_json({
                'https_url': https_url,
                'method': method,
                'note': 'Mac ã§ "tailscale serve --bg 8899" ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã“ã®URLã§PWAã«æ¥ç¶šã§ãã¾ã™',
            })
        else:
            self._send_json({
                'https_url': None,
                'method': 'not-found',
                'note': 'Tailscale ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ã‹ã€èµ·å‹•ã—ã¦ã„ã¾ã›ã‚“',
            })

    def _handle_tunnel_url(self):
        """Cloudflare Quick Tunnel ã®ç¾åœ¨ URL ã‚’è¿”ã™ã€‚"""
        global _CF_TUNNEL_URL
        if _CF_TUNNEL_URL:
            self._send_json({'url': _CF_TUNNEL_URL, 'active': True})
        else:
            self._send_json({'url': None, 'active': False,
                             'note': 'Cloudflare Tunnel ã¯æœªèµ·å‹•ã§ã™'})

    # ===== Docker ç®¡ç† =========================================================

    _DOCKER_BIN = '/usr/local/bin/docker'

    def _run_docker(self, args: list, timeout: int = 10) -> tuple[int, str, str]:
        """docker ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ (returncode, stdout, stderr) ã‚’è¿”ã™ã€‚"""
        try:
            r = subprocess.run(
                [self._DOCKER_BIN] + args,
                capture_output=True, text=True, timeout=timeout
            )
            return r.returncode, r.stdout, r.stderr
        except FileNotFoundError:
            return -1, '', 'docker command not found'
        except subprocess.TimeoutExpired:
            return -1, '', 'docker command timed out'

    def _handle_docker_containers(self):
        """å…¨ã‚³ãƒ³ãƒ†ãƒŠã®æƒ…å ±ã‚’è¿”ã™ (running + stopped)ã€‚"""
        rc, out, err = self._run_docker([
            'ps', '-a',
            '--format',
            '{{.ID}}\t{{.Names}}\t{{.Image}}\t{{.Status}}\t{{.Ports}}\t{{.State}}'
        ])
        if rc != 0:
            self._send_json({'error': err or 'docker ps failed'}, 500)
            return
        containers = []
        for line in out.strip().splitlines():
            parts = line.split('\t')
            if len(parts) < 6:
                continue
            containers.append({
                'id':     parts[0],
                'name':   parts[1],
                'image':  parts[2],
                'status': parts[3],
                'ports':  parts[4],
                'state':  parts[5],
            })
        self._send_json({'containers': containers})

    def _handle_docker_images(self):
        """ã‚¤ãƒ¡ãƒ¼ã‚¸ä¸€è¦§ã‚’è¿”ã™ã€‚"""
        rc, out, err = self._run_docker([
            'images',
            '--format',
            '{{.ID}}\t{{.Repository}}\t{{.Tag}}\t{{.Size}}\t{{.CreatedSince}}'
        ])
        if rc != 0:
            self._send_json({'error': err or 'docker images failed'}, 500)
            return
        images = []
        for line in out.strip().splitlines():
            parts = line.split('\t')
            if len(parts) < 5:
                continue
            images.append({
                'id':      parts[0],
                'repo':    parts[1],
                'tag':     parts[2],
                'size':    parts[3],
                'created': parts[4],
            })
        self._send_json({'images': images})

    def _handle_docker_logs(self, params: dict):
        """æŒ‡å®šã‚³ãƒ³ãƒ†ãƒŠã®ç›´è¿‘ãƒ­ã‚°ã‚’è¿”ã™ã€‚"""
        cid = params.get('id', [''])[0].strip()
        lines = params.get('lines', ['100'])[0]
        if not cid or not cid.replace('-', '').isalnum():
            self._send_json({'error': 'invalid id'}, 400)
            return
        try:
            n = max(1, min(int(lines), 500))
        except ValueError:
            n = 100
        rc, out, err = self._run_docker(['logs', '--tail', str(n), cid], timeout=15)
        # docker logs writes to stderr for actual log content in some versions
        log_text = out + (('\n' + err) if err and rc == 0 else '')
        if rc != 0:
            self._send_json({'error': err or 'docker logs failed'}, 500)
            return
        self._send_json({'id': cid, 'logs': log_text})

    def _handle_docker_action(self):
        """ã‚³ãƒ³ãƒ†ãƒŠæ“ä½œ: start / stop / restart / rmã€‚"""
        body = self._read_body()
        try:
            data = json.loads(body)
        except Exception:
            self._send_json({'error': 'invalid JSON'}, 400)
            return
        action = data.get('action', '')
        cid    = data.get('id', '').strip()
        if not cid or not cid.replace('-', '').isalnum():
            self._send_json({'error': 'invalid id'}, 400)
            return
        if action not in ('start', 'stop', 'restart', 'rm'):
            self._send_json({'error': 'invalid action'}, 400)
            return
        args = [action]
        if action == 'rm':
            args.append('-f')
        args.append(cid)
        rc, out, err = self._run_docker(args, timeout=30)
        if rc != 0:
            self._send_json({'error': err or f'docker {action} failed'}, 500)
        else:
            self._send_json({'ok': True, 'action': action, 'id': cid})

    # ===== iCloud ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç† =============================================

    def _handle_backup_list(self):
        """iCloud Backup/ ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§ã‚’è¿”ã™"""
        try:
            backups = []
            # Backup/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§
            if ICLOUD_BACKUP_DIR.exists():
                for f in sorted(ICLOUD_BACKUP_DIR.glob('attendance_backup_*.json'),
                                key=lambda p: p.stat().st_mtime, reverse=True):
                    stat = f.stat()
                    backups.append({
                        'name':    f.name,
                        'size_kb': round(stat.st_size / 1024, 1),
                        'mtime':   _dt.fromtimestamp(stat.st_mtime).isoformat(),
                    })
            # ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±
            main_info = None
            main_path = ICLOUD_ATT_DIR / 'attendance_backup.json'
            if main_path.exists():
                stat = main_path.stat()
                main_info = {
                    'size_kb': round(stat.st_size / 1024, 1),
                    'mtime':   _dt.fromtimestamp(stat.st_mtime).isoformat(),
                }
            # jinjer ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
            jinjer_files = []
            if ICLOUD_JINJER_DIR.exists():
                for f in sorted(ICLOUD_JINJER_DIR.glob('*.json'),
                                key=lambda p: p.stat().st_mtime, reverse=True):
                    stat = f.stat()
                    jinjer_files.append({
                        'name':    f.name,
                        'size_kb': round(stat.st_size / 1024, 1),
                        'mtime':   _dt.fromtimestamp(stat.st_mtime).isoformat(),
                    })
            self._send_json({
                'icloud_dir':    str(ICLOUD_ATT_DIR),
                'main':          main_info,
                'backups':       backups,
                'jinjer_files':  jinjer_files,
            })
        except Exception as e:
            self._send_json({'error': str(e)}, 500)

    def _handle_backup_read(self, params: dict):
        """æŒ‡å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è¿”ã™"""
        name = params.get('file', [''])[0]
        if not name:
            self._send_json({'error': 'file ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™'}, 400)
            return
        # ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«é˜²æ­¢
        if '/' in name or '\\' in name or name.startswith('.'):
            self._send_json({'error': 'ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ã™'}, 403)
            return
        target = ICLOUD_BACKUP_DIR / name
        if not target.exists():
            # ãƒ¡ã‚¤ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚‚è©¦ã™
            target = ICLOUD_ATT_DIR / name
        if not target.exists() or not target.suffix == '.json':
            self._send_json({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}, 404)
            return
        try:
            data = json.loads(target.read_text(encoding='utf-8'))
            months = list(data.get('months', {}).keys())
            self._send_json({
                'name':   name,
                'data':   data,
                'months': months,
                'mtime':  _dt.fromtimestamp(target.stat().st_mtime).isoformat(),
            })
        except Exception as e:
            self._send_json({'error': str(e)}, 500)

    def _handle_backup_now(self):
        """æ‰‹å‹•ã§ iCloud ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä»Šã™ãå®Ÿè¡Œã™ã‚‹"""
        try:
            if not KINTAI_DATA_FILE.exists():
                self._send_json({'error': 'kintai_store.json ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ãã ã•ã„ã€‚'}, 404)
                return
            data = json.loads(KINTAI_DATA_FILE.read_text(encoding='utf-8'))
            result = _icloud_backup(data, label='manual')
            self._send_json(result)
        except Exception as e:
            self._send_json({'error': str(e)}, 500)

    def _handle_backup_restore(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ kintai ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒã™ã‚‹ (data/kintai_store.json ã‚’ä¸Šæ›¸ã)"""
        body = self._read_body()
        file_name = body.get('file', '') if body else ''
        if not file_name:
            # ãƒ•ã‚¡ã‚¤ãƒ«åæœªæŒ‡å®šæ™‚ã¯ attendance_backup.json (æœ€æ–°) ã‹ã‚‰å¾©å…ƒ
            src = ICLOUD_ATT_DIR / 'attendance_backup.json'
        else:
            if '/' in file_name or '\\' in file_name:
                self._send_json({'error': 'ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ã™'}, 403)
                return
            src = ICLOUD_BACKUP_DIR / file_name
        if not src.exists():
            self._send_json({'error': f'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {src.name}'}, 404)
            return
        try:
            data = json.loads(src.read_text(encoding='utf-8'))
            months = list(data.get('months', {}).keys())
            if not months:
                self._send_json({'error': 'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã«æœˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“'}, 400)
                return
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰ä¸Šæ›¸ã
            if KINTAI_DATA_FILE.exists():
                bak = KINTAI_DATA_FILE.with_suffix(f'.bak{int(time.time())}')
                KINTAI_DATA_FILE.rename(bak)
            data['_server_saved_at']    = _dt.now().isoformat()
            data['_restored_from']      = src.name
            data['_restored_at']        = _dt.now().isoformat()
            KINTAI_DATA_FILE.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
            self._send_json({
                'ok':     True,
                'source': src.name,
                'months': months,
                'saved_at': data['_server_saved_at'],
            })
        except Exception as e:
            self._send_json({'error': str(e)}, 500)

    def _handle_system_restart(self):
        """launchd çµŒç”±ã§ã‚µãƒ¼ãƒãƒ¼è‡ªèº«ã‚’1ç§’å¾Œã«å†èµ·å‹•ã™ã‚‹"""
        def _do_restart():
            import time as _t
            _t.sleep(1)
            uid = subprocess.run(['id', '-u'], capture_output=True, text=True).stdout.strip()
            subprocess.run(
                ['launchctl', 'kickstart', '-k', f'gui/{uid}/com.kintai.server'],
                capture_output=True,
            )
        threading.Thread(target=_do_restart, daemon=True, name='restart-thread').start()
        self._send_json({'status': 'restarting', 'message': '1ç§’å¾Œã«å†èµ·å‹•ã—ã¾ã™'})


def _server_is_alive(port: int) -> bool:
    """ãƒãƒ¼ãƒˆã§è‡ªã‚µãƒ¼ãƒãƒ¼ãŒå¿œç­”ã™ã‚‹ã‹ç¢ºèª (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ 2 ç§’)"""
    try:
        req = urllib.request.Request(f'http://127.0.0.1:{port}/api/structure')
        with urllib.request.urlopen(req, timeout=2):
            return True
    except Exception:
        return False


def _kill_port(port: int) -> bool:
    """æŒ‡å®šãƒãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å…¨ãƒ—ãƒ­ã‚»ã‚¹ã« SIGTERM ã‚’é€ã‚‹ã€‚æˆåŠŸã—ãŸã‚‰ True ã‚’è¿”ã™"""
    try:
        r = subprocess.run(['lsof', '-ti', f':{port}'], capture_output=True, text=True)
        pids = [p.strip() for p in r.stdout.strip().splitlines() if p.strip()]
        if not pids:
            return False
        for pid in pids:
            try:
                os.kill(int(pid), _signal.SIGTERM)
                print(f'  æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ (PID {pid}) ã‚’çµ‚äº†ã—ã¾ã—ãŸ')
            except ProcessLookupError:
                pass
        time.sleep(2.0)  # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚’å¾…ã¤
        return True
    except Exception as ex:
        print(f'  ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚¨ãƒ©ãƒ¼: {ex}')
        return False


_BANNER = f"""jinjeråŒæœŸã‚µãƒ¼ãƒãƒ¼èµ·å‹•: http://0.0.0.0:{{port}}
  ğŸ“± PWA ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ã‚¯ã‚»ã‚¹: http://localhost:{{port}}/
     â†‘ Safari ã§ã¯ GitHub Pages(HTTPS) ã‹ã‚‰ã¯ localhost:8899 ã«æ¥ç¶šã§ããªã„ãŸã‚
       ã“ã®URLã‚’ç›´æ¥ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã—ã¦ãã ã•ã„ã€‚
  GET  /api/health                      â€” ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
  GET  /api/jinjer?months=2026-02       â€” jinjer å‹¤æ€ ãƒ‡ãƒ¼ã‚¿å–å¾—
  GET  /api/reports                     â€” ä½œæ¥­å ±å‘Šæ›¸ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
  GET  /api/reports/read?year=2026&month=02 â€” æŒ‡å®šæœˆ Excel â†’ JSON
  POST /api/reports/sync                â€” kintai ãƒ‡ãƒ¼ã‚¿ â†’ Excel æ›¸ãè¾¼ã¿
  POST /api/reports/generate            â€” ç¿Œæœˆ Excel è‡ªå‹•ç”Ÿæˆ
  GET  /api/structure                   â€” STRUCTURE.md ã®å†…å®¹
  GET  /api/jobs?categories=1,2,3&keywords=Python â€” ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹æ¡ˆä»¶ãƒ•ã‚£ãƒ¼ãƒ‰
Ctrl+C ã§çµ‚äº†"""


if __name__ == '__main__':
    # launchd ç®¡ç†ä¸‹ã‹ã©ã†ã‹ã‚’æ¤œå‡º
    # æ–¹æ³•1: plist ã® EnvironmentVariables ã§æ˜ç¤ºè¨­å®šã—ãŸ KINTAI_MANAGED ãƒ•ãƒ©ã‚°
    # æ–¹æ³•2: è¦ªãƒ—ãƒ­ã‚»ã‚¹ãŒ launchd (PID 1) ã‹ã©ã†ã‹
    _IS_LAUNCHD = bool(os.environ.get('KINTAI_MANAGED')) or (os.getppid() == 1)

    # â”€â”€â”€ ã‚¹ãƒ†ãƒƒãƒ—1: æ—¢ã«è‡ªã‚µãƒ¼ãƒãƒ¼ãŒç¨¼åƒã—ã¦ã„ã‚‹ã‹ç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # launchd èµ·å‹•æ™‚ã¯ã“ã®ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—:
    #   ãƒã‚§ãƒƒã‚¯ãŒ True â†’ sys.exit(0) â†’ launchd ãŒ KeepAlive ã§å†èµ·å‹• â†’ ç„¡é™ãƒ«ãƒ¼ãƒ—
    # æ‰‹å‹•èµ·å‹•æ™‚ã®ã¿ã€Œæ—¢ã«ç¨¼åƒä¸­ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†ã™ã‚‹ã€‚
    if not _IS_LAUNCHD and _server_is_alive(PORT):
        print(f'[OK] jinjeråŒæœŸã‚µãƒ¼ãƒãƒ¼ã¯ãƒãƒ¼ãƒˆ {PORT} ã§æ—¢ã«ç¨¼åƒä¸­ã§ã™ï¼ˆlaunchd å¸¸é§ï¼‰ã€‚')
        print(f'  å†èµ·å‹•ãŒå¿…è¦ãªå ´åˆ:')
        print(f'    launchctl stop com.kintai.server')
        print(f'    launchctl unload ~/Library/LaunchAgents/com.kintai.server.plist')
        print(f'    python3 jinjer_server.py')
        sys.exit(0)

    # â”€â”€â”€ ã‚¹ãƒ†ãƒƒãƒ—2: launchd èµ·å‹•æ™‚ã¯æ—§ãƒ—ãƒ­ã‚»ã‚¹ã‚’å…ˆã«æƒé™¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ç›´å¾Œã¯æ—§ã‚½ã‚±ãƒƒãƒˆãŒã¾ã æ®‹ã£ã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€å…ˆã«ãƒãƒ¼ãƒˆã‚’è§£æ”¾ã™ã‚‹ã€‚
    if _IS_LAUNCHD:
        _kill_port(PORT)

    # â”€â”€â”€ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒã‚¤ãƒ³ãƒ‰ã‚’è©¦ã¿ã‚‹ï¼ˆSO_REUSEADDR ã§ TIME_WAIT ã‚’å›é¿ï¼‰â”€â”€â”€â”€
    server = None
    for attempt in range(3):
        try:
            server = ReuseHTTPServer(('0.0.0.0', PORT), JinjerHandler)
            break
        except OSError as e:
            if e.errno != _errno.EADDRINUSE:
                raise
            if attempt >= 2:
                print(f'[ERROR] ãƒãƒ¼ãƒˆ {PORT} ã®è§£æ”¾ã«å¤±æ•—ã—ã¾ã—ãŸã€‚')
                print(f'  æ‰‹å‹•ã§ç¢ºèª: lsof -ti :{PORT}')
                sys.exit(1)
            # ã‚¾ãƒ³ãƒ“ã‚½ã‚±ãƒƒãƒˆã‚„ä»–ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ’é™¤ã—ã¦å†è©¦è¡Œ
            print(f'[è­¦å‘Š] ãƒãƒ¼ãƒˆ {PORT} ãŒä½¿ç”¨ä¸­ã§ã™ã€‚æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã—ã¦å†è©¦è¡Œã—ã¾ã™... ({attempt+1}/2)')
            _kill_port(PORT)

    print(_BANNER.format(port=PORT))

    # â”€â”€â”€ ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ & æ—¥ä»˜ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _LOG_MAX_BYTES        = 2 * 1024 * 1024   # 2MB è¶…ãˆãŸã‚‰ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
    _LOG_KEEP             = 5                  # ã‚µã‚¤ã‚ºãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¸–ä»£æ•° (.1ã€œ.5)
    _LOG_ROTATE_INTERVAL  = 3600               # 1æ™‚é–“ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    _LOG_DIR              = _HERE / 'logs'
    _LOG_ARCHIVE_DIR      = _HERE / 'logs' / 'dates'  # æ—¥ä»˜åˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
    _LOG_ARCHIVE_KEEP_DAYS = 90               # ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ä¿æŒæœŸé–“ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯ç¯€ç´„ï¼‰

    def _rotate_one(log_path: Path):
        """1ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: path.5 ã‚’å‰Šé™¤ â†’ .4â†’.5 â€¦ â†’ pathâ†’.1"""
        try:
            if not log_path.exists() or log_path.stat().st_size < _LOG_MAX_BYTES:
                return
            # æœ€å¤ä¸–ä»£ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ã‚·ãƒ•ãƒˆ
            oldest = log_path.with_suffix(f'.log.{_LOG_KEEP}')
            if oldest.exists():
                oldest.unlink()
            for i in range(_LOG_KEEP - 1, 0, -1):
                src = log_path.with_suffix(f'.log.{i}')
                dst = log_path.with_suffix(f'.log.{i+1}')
                if src.exists():
                    src.rename(dst)
            # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ .1 ã«ç§»å‹•ï¼ˆlaunchd ã¯æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã«è‡ªå‹•ã§æ›¸ãç¶šã‘ã‚‹ï¼‰
            log_path.rename(log_path.with_suffix('.log.1'))
            print(f'[INFO] ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: {log_path.name} â†’ .1 ({log_path.stat().st_size if log_path.exists() else "æ–°è¦"})', flush=True)
        except Exception as ex:
            print(f'[WARN] ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•— ({log_path.name}): {ex}', flush=True)

    def _rotate_logs():
        """å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        for name in ('server.log', 'server_err.log', 'watchdog.log', 'watchdog_err.log',
                     'jinjer_end.log', 'jinjer_end_err.log'):
            _rotate_one(_LOG_DIR / name)

    def _rotate_loop():
        """èµ·å‹•æ™‚ã«1å› + ä»¥é™1æ™‚é–“ã”ã¨ã«å®Ÿè¡Œ"""
        _rotate_logs()
        while True:
            time.sleep(_LOG_ROTATE_INTERVAL)
            _rotate_logs()

    threading.Thread(target=_rotate_loop, daemon=True, name='log-rotator').start()

    # â”€â”€â”€ æ—¥æ¬¡ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– (SysLog HA: 90æ—¥ä¿æŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _archive_logs_daily():
        """å‰æ—¥ã® server.log, watchdog.log ã‚’ logs/dates/YYYY-MM-DD/ ã«ã‚³ãƒ”ãƒ¼"""
        try:
            yesterday = (_dt.now() - _td(days=1)).strftime('%Y-%m-%d')
            arc_dir = _LOG_ARCHIVE_DIR / yesterday
            arc_dir.mkdir(parents=True, exist_ok=True)
            for name in ('server.log', 'watchdog.log'):
                src_f = _LOG_DIR / name
                if src_f.exists():
                    shutil.copy2(src_f, arc_dir / name)
            print(f'[INFO] æ—¥æ¬¡ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†: {arc_dir}', flush=True)
            # 90æ—¥ä»¥ä¸Šå‰ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’å‰Šé™¤
            cutoff = _dt.now() - _td(days=_LOG_ARCHIVE_KEEP_DAYS)
            if _LOG_ARCHIVE_DIR.exists():
                for d in list(_LOG_ARCHIVE_DIR.iterdir()):
                    try:
                        if d.is_dir() and _dt.strptime(d.name, '%Y-%m-%d') < cutoff:
                            shutil.rmtree(d)
                            print(f'[INFO] å¤ã„ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’å‰Šé™¤: {d.name}', flush=True)
                    except Exception: pass
        except Exception as ex:
            print(f'[WARN] æ—¥æ¬¡ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¤±æ•—: {ex}', flush=True)

    def _daily_archive_loop():
        """èµ·å‹•æ™‚ã«1å›å®Ÿè¡Œã€ä»¥é™ã¯æ¯æ—¥0æ™‚30ç§’ã«å®Ÿè¡Œ"""
        _archive_logs_daily()
        while True:
            now = _dt.now()
            nxt = (now + _td(days=1)).replace(hour=0, minute=0, second=30, microsecond=0)
            time.sleep(max(1, (nxt - _dt.now()).total_seconds()))
            _archive_logs_daily()

    threading.Thread(target=_daily_archive_loop, daemon=True, name='daily-archiver').start()

    # â”€â”€â”€ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ STRUCTURE.md ã‚’æœ€æ–°åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _update_structure():
        try:
            r = subprocess.run(
                [sys.executable, str(_HERE / 'generate_structure.py')],
                cwd=str(_HERE), capture_output=True, text=True, timeout=30
            )
            if r.returncode == 0:
                print('[INFO] STRUCTURE.md ã‚’æ›´æ–°ã—ã¾ã—ãŸ', flush=True)
            else:
                print(f'[WARN] STRUCTURE.md æ›´æ–°å¤±æ•—: {r.stderr.strip()}', flush=True)
        except Exception as e:
            print(f'[WARN] STRUCTURE.md æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}', flush=True)

    threading.Thread(target=_update_structure, daemon=True, name='structure-updater').start()

    # â”€â”€â”€ Cloudflare Quick Tunnel è‡ªå‹•èµ·å‹• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _start_cloudflare_tunnel():
        """cloudflared quick tunnel ã‚’èµ·å‹•ã—ã¦ URL ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚
        tunnel URL ã¯ trycloudflare.com ã®ä¸€æ™‚ URLï¼ˆMac å†èµ·å‹•ã”ã¨ã«å¤‰ã‚ã‚‹ï¼‰ã€‚
        """
        global _CF_TUNNEL_URL, _CF_TUNNEL_PROC
        import shutil as _shutil
        # launchd ã¯ /usr/local/bin ã‚’ PATH ã«å«ã¾ãªã„ãŸã‚ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æ¢ã™
        _CF_CANDIDATES = [
            '/usr/local/bin/cloudflared',
            '/opt/homebrew/bin/cloudflared',
            '/usr/bin/cloudflared',
        ]
        cloudflared = _shutil.which('cloudflared') or next(
            (p for p in _CF_CANDIDATES if Path(p).exists()), None
        )
        if not cloudflared:
            print('[INFO] cloudflared ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Cloudflare Tunnel ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚', flush=True)
            return

        # æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ãŒå‹•ã„ã¦ã„ã‚Œã°åœæ­¢
        if _CF_TUNNEL_PROC and _CF_TUNNEL_PROC.poll() is None:
            _CF_TUNNEL_PROC.terminate()

        print('[INFO] Cloudflare Quick Tunnel ã‚’èµ·å‹•ä¸­...', flush=True)
        try:
            proc = subprocess.Popen(
                [cloudflared, 'tunnel', '--url', f'http://localhost:{PORT}',
                 '--no-autoupdate'],
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                text=True, bufsize=1
            )
            _CF_TUNNEL_PROC = proc

            # ãƒ­ã‚°ã‚’èª­ã‚“ã§ URL ã‚’æŠ½å‡º
            url_pattern = re.compile(r'https://[a-z0-9\-]+\.trycloudflare\.com')
            for line in proc.stdout:
                line = line.rstrip()
                m = url_pattern.search(line)
                if m:
                    _CF_TUNNEL_URL = m.group(0)
                    print(f'[INFO] Cloudflare Tunnel URL: {_CF_TUNNEL_URL}', flush=True)
                    break  # URL å–å¾—å¾Œã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å‹•ã‹ã—ç¶šã‘ã‚‹

            # stdout ã‚’å¼•ãç¶šãèª­ã¿ç¶šã‘ã‚‹ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ç”Ÿã‹ã™ãŸã‚ï¼‰
            for _ in proc.stdout:
                pass

        except Exception as ex:
            print(f'[WARN] Cloudflare Tunnel èµ·å‹•å¤±æ•—: {ex}', flush=True)

    threading.Thread(target=_start_cloudflare_tunnel, daemon=True, name='cf-tunnel').start()

    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print('\nã‚µãƒ¼ãƒãƒ¼ã‚’åœæ­¢ã—ã¾ã—ãŸ')
        if _CF_TUNNEL_PROC and _CF_TUNNEL_PROC.poll() is None:
            _CF_TUNNEL_PROC.terminate()
